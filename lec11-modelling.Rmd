---
title: "Statistical/probabilistic modelling in R"
author: "Luke Johnston"
---

## Lesson preamble

> ### Lesson objectives:
> 
> - Learn how to apply and interpret linear regression for a variety of data
> - Understand probability and the importance of presenting confidence intervals
> - Learn the importance of visualizing your data when doing any analyses or
> statistics
> - Learn how to compare models using information criterion techniques
> 
> ### Lesson outline:
> 
> - Statistics and probability
> - Linear regression 
>     - Types of dependent ($y$) data
>     - Types of independent ($x$) data
> - Estimation vs hypothesis testing
> - Model comparison techniques
> 
> ### Setup
>
> - `install.packages("dplyr")` # or `tidyverse`
> - `install.packages("tidyr")` # or `tidyverse`
> - `install.packages("broom")`
> - `install.packages("MuMIn")`
> - `install.packages("datasauRus")`

-----

```{r setup, results='hide'}
library(tidyverse)
library(broom)
library(datasauRus)
library(MuMIn)
```

## Statistics and probability

- Theoretical models are powerful tools at explaining or understanding the
world. However, they are limited in that the real-world often doesn't perfectly
fit these models. The real world is messy and noisy. We use statistics and
probability to determine whether what we are studying is a really



## Generalized linear models

Generalized linear models (or GLM) is statistical modelling technique that
encompasses several other techniques, e.g. correlation or analysis of variance
(ANOVA), linear regression, logistic regression. GLM is a powerful technique
that you can use on a wide range of data and research questions, and is the
foundation for understanding how more advanced techniques work. This is the
reason I will be covering GLM in depth. (For those who will go into graduate
students or into research in general, you will likely use mixed effects models.
If you can understand and grasp GLM, it will be substantially easier for you to
understand mixed effects modelling and other techniques.)

A version of GLM that uses continuous $y$ values is called linear regression,
which I'm going to focus on. The formula for linear regression (or GLM in
general) is:

$$ Y = \alpha + X\beta + \varepsilon $$

Or, a simplified, alphabetized version is:

$$ y = a + Xb + e $$

In the case of multiple linear regression (more than one $x$), it is expanded to:

$$ y = a + x_1b_1 + x_2b_b +...+ x_nb_n + e $$

Where each $x$ is a variable in a data frame. In R we can run linear regression
either using `lm` or using `glm`. We'll use `glm` since it can be expanded to
other types of analyses. First, let's load up a dataset.

```{r}
portal <- read_csv("portal_data.csv")
```

```{r}
fit1 <- glm(weight ~ sex, data = portal, family = gaussian)
summary(fit1)
```

There's a lot of results and information contained within `fit1`, which
`summary()` extracts and presents in a fairly nice format. However, a lot of
this information we aren't really interested in. So let's extract only what we
want, using the `broom` package. (get it, broom to clean up?)

```{r}
library(broom)
tidy(fit1)
```

Much nicer! Ok, so what does this actually mean? Let's go back to the equation
and convert the above into the formula:

$$ weight = Intercept + (sexM \times b) $$

Let's substitute in the numbers. The intercept, is well, the intercept in the
data above. The $b$ is the estimate for the `year`.

$$ weight = 42.17 + (sexM \times 0.825) $$

We can use this equation to estimate the value of weight for a specific sex. But
first, let's make sure this is working correctly. Let's find out the weight at
the very first collection visit.

```{r}
portal %>% 
    filter(!is.na(sex)) %>% 
    group_by(sex) %>% 
    summarize(mean_wt = mean(weight, na.rm = TRUE)) 
```

Ok, let's compare to the equation. The variable `sexM` is 1 for male and 0 for female. So if we want to calculate the weight of female, we have to set `sexM` as 0 in the equation. Let's do it:

```{r}
42.17 + (0 * 0.825)
```

Same as above! And for males (equal to 1):

```{r}
42.17 + (1 * 0.825)
```

Good, it matches what we calculated. Why is this useful? Well, we can add more
terms to the GLM equation. This allows us to determine how a variable influences
the $y$ when other variables are held constant and is known as *multiple linear
regression*. Since weight is probably also dependent on `hindfoot_length`, let's
add that to the model and put the values into the equation.

```{r}
fit2 <- glm(weight ~ sex + hindfoot_length, data = portal)
tidy(fit2)
```

$$ weight = -32.09 + (sexM \times -1.72) + (hindfoot\_length \times 2.56)$$

Hmm, that's interesting... is a *negative* intercept... How can weight be
negative? Well, that's because the intercept is calculated by setting *both* sex
and hindfoot length to zero! But is it physically even possible that a hindfoot be
non-existant?? No. But math doesn't care what is physically possible or not. So
let's change that up by changing the actual value of hindfoot length in the
dataset. There are several ways to transform the data into something that is
meaningful to us. But first:

> What does everyone think is a meaningful way to transform hindfoot length so
we can interpret the model more appropriately?

There are many ways to do it, but one way that is often used is to mean center
the data. Mean centering is substracting each value by the mean. After mean
centering, 0 is equal to the mean of hindfoot length.

```{r}
portal2 <- portal %>%
    mutate(hfl_mean_center = hindfoot_length - mean(hindfoot_length, na.rm = TRUE))

fit3 <- glm(weight ~ sex + hfl_mean_center, data = portal2)
tidy(fit3)
```

$$ weight = 42.88 + (sexM \times -1.72) + (hfl\_mean\_center \times 2.56)$$

Much better. So if we wanted to compare males vs females who have a mean
hindfoot length (set to 0):

```{r}
# Female
42.88 + (0 * -1.72) + (0 * 2.56)
# Male
42.88 + (1 * -1.72) + (0 * 2.56)
```
Or if we wanted to see males with hindfeet 10 cm minus the mean vs 10 cm plus the mean:

```{r}
# Males 10 cm minus mean
42.88 + (1 * -1.72) + (-10 * 2.56)
# Male 10 cm plus mean
42.88 + (1 * -1.72) + (10 * 2.56)
```

#### Challenge

1. Create a new dataframe called `challenge1` that keeps only taxa of "Rodent"
(`filter`) and with a new column (`mutate`) called "Dipodomys" where if genus
is equal to "Dipodomys" than the value is "Yes" and if not the value is "No"
(hint: use `ifelse`). Then, create a model called `fit_challenge1` that has the
terms `sex`, `hfl_mean_center`, and `Dipodomys`. Extract the relevant
information from the model using `tidy`. What is the weight when sex is female,
hindfoot length is 5 above the mean, and Dipodomys is yes?

```{r, echo=FALSE, eval=FALSE}
challenge1 <- portal2 %>% 
    filter(taxa == "Rodent") %>% 
    mutate(Dipodomys = ifelse(genus == "Dipodomys", "Yes", "No"))

fit_challenge1 <- glm(weight ~ sex + hfl_mean_center + Dipodomys, data = challenge1)
tidy(fit_challenge1)
```

2. Use the below code to create a new variable `Disease` with (random) values as
either 0 for "Healthy" or 1 for "Diseased". Then, write up a formula using `glm`
to analyze the role that weight, sex, and hindfoot length (*not* mean centered)
has on disease status. Because the $y$ is not continuous, you need to
set `family = binomial`. Run the code, check the summary (or `tidy`), put the
numbers into an equation (as we did above) and *try* to interpret the results
(where sex is female, weight is 50, and hindfoot length is 30).

```{r}
challenge2 <- portal %>% 
    # rbinom randomly creates values for binary data.
    mutate(Disease = rbinom(nrow(.), 1, 0.25))
```

```{r, echo=FALSE, eval=FALSE}
fit_challenge2 <- glm(Disease ~ weight + sex + hfl_mean_center, 
                      data = challenge2, family = binomial)
summary(fit_challenge2)
tidy(fit_challenge2)

cor(portal$weight, portal$hindfoot_length, use = "complete.obs")
```

### Logistic regression

### Interaction?

### Example: Using linear regression in a Ricker model?

## Estimation, confidence intervals, and p-values

What's the point of using `glm`? Not only does it create a model with constants
and coefficients, but it is also used to calculate how *certain* we are about
the results. The main power of GLM comes from the ability to estimate model
parameters and derive meaning from how certain $x$ variables (independent
variables) influence the $y$ (dependent variable). With his model we can use to
it *predict* what could happen to some value if we knew or could change the 
independent variables. 

However, from GLM we can determine 

```{r}
fit_hf <- glm(weight ~ hindfoot_length, data = portal)
tidy(fit_hf, conf.int = TRUE)
```

```{r}
fit_hfsex <- glm(weight ~ hindfoot_length + sex, data = portal)
tidy(fit_hfsex, conf.int = TRUE)
```


<!-- how many know about p-values etc? -->

## Model comparison techniques

### Information criterion

```{r}
library(MuMIn)

fit1 <- glm(weight ~ hindfoot_length, data = portal)
# update uses the `fit1` model and just adds terms to it.
fit2 <- update(fit1, . ~ . + sex)
fit3 <- update(fit1, . ~ . + sex + species)
fit4 <- glm(log(weight) ~ hindfoot_length + sex, data = portal)
# scale with scale = FALSE is mean-centering the data. It does the same thing as value - mean
fit5 <- glm(log(weight) ~ scale(hindfoot_length, scale = FALSE) + sex, data = portal)
fit6 <- glm(log(weight) ~ scale(hindfoot_length, scale = FALSE) + sex + species, data = portal)
fit7 <- glm(log(weight) ~ scale(hindfoot_length, scale = FALSE) + sex + species + year, data = portal)

model.sel(fit1, fit2, fit3, fit4, fit5, fit6, fit7)
```


## Visualization as a crucial component to data analysis

Visualizing your data before, during, and after doing statistics or modelling
your data is an incredibly important and sometimes overlooked aspect of data
analysis. Sometimes statistics and modelling give you an answer that you sort of
expect, but the actual data is telling a different story.

#### Challenge

There are several datasets found within the `datasauRus` package. This package
is trying to illustrate a point. First, let's look at the mean and standard
deviation of the variables in each dataset:

```{r}
library(datasauRus)
library(tidyverse)
library(broom)
datasaurus_dozen %>%
    group_by(dataset) %>%
    summarise(
        mean_x = mean(x),
        sd_x = sd(x),
        mean_y = mean(y),
        sd_y = sd(y)
    )
```

They're all basically the same... What about for linear regression?

```{r}
# Linear regression on each dataset
datasaurus_dozen %>% 
    group_by(dataset) %>% 
    do(tidy(lm(y ~ x, data = .))[2, ])
```

Same thing, all are more or less the same. But, there is something seriously
wrong with every since one of the datasets. Pair up and explore the data
visually to see what's going on.

```{r, echo=FALSE, eval=FALSE}
ggplot(datasaurus_dozen, aes(x = x, y = y, colour = dataset)) +
    geom_point() +
    theme_void() +
    theme(legend.position = "none") +
    facet_wrap(~ dataset, ncol = 3)
```


### Other examples of finding a problem by visualizing

From what you found in the challenge above, can you see why it's dangerous to
not visualize the data first? Sometimes

![Visualizing raw data vs aggregate](https://g.redditmedia.com/Opizo6PEpuT_cL0N0tWK5g59CsMHFystdNCpYOqhu-A.gif?w=884&fm=mp4&mp4-fragmented=false&s=222875e5455c31829929add6c426a86b)

Another example is known as Simpon's Paradox. You all have already encountered
Simpson's Paradox in the challenges of lecture 4 and 5, where the average weight
of all species decreased over time, but for each species weight remained
constant. These types of problems are encountered all the time in data analysis
and it is part of what makes science so hard, because data often doesn't behave
as we expect it to. The figure below illustrates Simpson's Paradox.

```{r}
ggplot(simpsons_paradox, aes(x = x, y = y, colour = dataset)) +
    geom_point() +
    geom_smooth(method = "lm") +
    theme(legend.position = "none") +
    facet_wrap( ~ dataset, ncol = 3)
```

And sometimes, by not looking at the raw data, it can lead to actual harm.
For instance, Simpson's paradox comes up in medical studies of drugs
that could be lifesaving for a patient. If inappropriate conclusions are drawn
because the data wasn't completely explored, people could be harmed or could
die. This is true in *any* scientific field, including ecology. Policies aimed
at benefitting the environment could in fact be *harming* it because these types
of things were not examined. 

[^simpson]: Wikipedia page on [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)

```{r}
library(MuMIn)
norm_data <- simpsons_paradox %>% 
    filter(dataset == "simpson_1")
fit_norm <- glm(y ~ x, data = norm_data)

sp_data <- simpsons_paradox %>% 
    filter(dataset == "simpson_2")
fit_sp <- glm(y ~ x, data = sp_data)

model.sel(fit_norm, fit_sp)
```

## Complete Survey?

Link

## Resources

- https://stats.stackexchange.com/questions/81000/calculate-coefficients-in-a-logistic-regression-with-r
- https://www.autodeskresearch.com/publications/samestats
