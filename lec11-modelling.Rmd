---
title: "Statistical/probabilistic modelling in R"
---

<!-- 
brainstorm:

- Some basics ?
    - mean, sd, median, IQR
        - plot *and* showing raw data/results (as table)

- how linear regression works on real world data
- How powerful linear regression is
    - use Ricker model as an example
- spurious (?) correlations (different data distribution that give the same estimate)

- p-values
    - unreliability
    - better to estimate than test significance
- CI


- Comparing models

Outline:

- why stats/probability
- Estimation, confidence intervals, and p-values
- Linear regression
    - End challenge: 
- Model selection
- Importance of plotting
    - Challenge: Show simple stats and simple linear regression of the various
    datasauRus packages. But there is a problem. Use visualization to find out
    what's wrong with the datasets.


-->

## Lesson preamble

> ### Lesson objectives:
> 
> - Learn the importance of visualizing your data when doing any analyses or
> statistics
> - Learn how to apply and interpret linear regression for a variety of data
> - Understand probability and the importance of presenting confidence intervals
> - Learn how to compare models using information criterion techniques
> 
> ### Lesson outline:
> 
> - Statistics and probability
> - Linear regression 
>     - Types of dependent ($y$) data
>     - Types of independent ($x$) data
> - Estimation vs hypothesis testing
> - Model comparison techniques
> 
> ### Setup
>
> - `install.packages("dplyr")`
> - `install.packages("tidyr")`
> - `install.packages("broom")`
> - `install.packages("datasauRus")`

-----

```{r}
library(tidyverse)
library(datasauRus)
library(broom)
```

## Statistics and probability

- Theoretical models are powerful tools at explaining or understanding the world. However, they are limited in that the real-world often doesn't perfectly fit these models. The real world is messy and noisy. We use statistics and probability to determine whether what we are studying is a really




## Estimation, confidence intervals, and p-values

P-values vs estimation

confidence interval

<!-- how many know about p-values etc? -->




## Generalized linear models

Generalized linear models (or GLM) is statistical modelling technique that
encompasses several other techniques, e.g. correlation or analysis of variance
(ANOVA). GLM is a powerful technique that you can use on a wide range of data
and research questions, and is the foundation for understanding how more
advanced techniques work. This is the reason I will be covering GLM in depth.
(For those who will go into graduate students or into research in general, you
will likely use mixed effects model. If you can understand and grasp GLM, it
will be substantially easier for you to understand mixed effects modeling and
other techniques.)

A version of GLM that uses continuous $y$ values is called linear regression,
which I'm going to focus on. The formula for linear regression (or GLM in
general) is:

$$ Y = \alpha + X\beta + \varepsilon $$

Or, a simplified, alphabetized version is:

$$ y = m + Xb + e $$

In the case of multiple linear regression (more than one $x$), it is expanded to:

$$ y = m + x_1b_1 + x_2b_b +...+ x_nb_n + e $$

In R we can run linear regression either using `lm` or using `glm`. We'll use
`glm` since it can be expanded to other types of analyses. First, let's load up
a dataset.

```{r}
library(tidyverse)

portal <- read_csv("portal_data.csv")
portal

fit1 <- glm(weight ~ year, data = portal, family = gaussian)
summary(fit1)

library(broom)
tidy(glm(weight ~ year, data = portal))
```

Ok, so what does this actually mean? Let's go back to the equation and convert
the above into the formula:

$$ weight = Intercept + (year \times b) $$

Let's substitute in the numbers. The intercept, is well, the intercept in the
data above. The $b$ is the estimate for the `year`.

$$ weight = 2752.13 + (year \times -1.36113) $$

We can use this equation to estimate the value of weight at a given year. But
first, let's make sure this is working correctly. Let's find out the weight at
the very first collection visit.

```{r}
portal %>% 
    filter(year == min(year)) %>%
    summarize(mean_wt = mean(weight, na.rm = TRUE)) %>% 
    as.numeric()
```

Ok, let's compare to the equation. Let's substitute year for the first year data
was collected, which was `r min(portal$year)`.

```{r}
2752.13 + (min(portal$year) * -1.36113)
```

```{r}
portal2 <- portal %>%
    mutate(year = year - min(year))

tidy(glm(weight ~ year, data = portal2, family = gaussian))
```



The above is for a simple, two variable linear regression. What about if we have 
multiple predictor ($x$) variables? We do:

```{r}
fit2 <- lm(Murder ~ Income + Population, data = ds)
summary(fit2)
```

Converted to the formula:

$$ Murder = 14.34 + (Income \times -0.0189) + (Population \times 0.0003384) $$

Nearly all statistical techniques (at least the basic/common ones) can be
considered a special case of linear regression. Let's take ANOVA as an example.
ANOVA is where the $y$ is a continuous variable and the $x$ is a discrete
variable (usually with at least three categories). In my opinion, using linear
regression is more useful from a scientific and interpretation perspective than
using ANOVA. Here's how you do it:

```{r}
fit3 <- lm(Petal.Length ~ Species, data = iris)
# To see the different categories of Species
summary(iris$Species)
```

```{r}
summary(fit3)
```

Converted to an equation. Setosa isn't here because it is considered the
'intercept' since if you set Versicolor and Virginica to zero, the remaining
'category' is Setosa. These are known as dummy variables.

$$ Petal.Length = 1.46 + (Versicolor \times 2.79) + (Virginica \times 4.09) $$

The estimates from the linear regression as the same as the means of each group
(each estimate + the intercept). 

```{r}
library(dplyr)
iris %>%
    group_by(Species) %>% 
    summarize(means = mean(Petal.Length))
```

Linear regression and correlation are also tightly linked, and are the same if
the variables have been scaled (mean-centered and standardized).

```{r}
ds <- iris %>%
    mutate_each(funs(scale), Petal.Length, Sepal.Length)
summary(ds)
```

```{r}
cor(ds$Petal.Length, ds$Sepal.Length)
```

```{r}
summary(lm(Petal.Length ~ Sepal.Length, data = ds))
```

Using linear regression to compare means between groups (e.g. species or sex)




### Example: Using linear regression in a Ricker model

## Model comparison techniques

### Information criterion

## Importance of visualizing the data directly

Visualizing your data before, during, and after doing statistics or modelling
your data is incredibly important. There 

#### Challenge

There are 
```{r}
# From the datasauRus package
datasaurus_dozen %>%
    group_by(dataset) %>%
    summarise(
        mean_x = mean(x),
        sd_x = sd(x),
        mean_y = mean(y),
        sd_y = sd(y)
    )

# Linear regression on each dataset
datasaurus_dozen %>% 
    group_by(dataset) %>% 
    do(tidy(lm(y ~ x, data = .))[2, ])
```




```{r, echo=FALSE, eval=FALSE}
ggplot(datasaurus_dozen, aes(x = x, y = y, colour = dataset)) +
    geom_point() +
    theme_void() +
    theme(legend.position = "none") +
    facet_wrap(~ dataset, ncol = 3)
```

### Other examples of finding a problem by visualizing

![Visualizing raw data vs aggregate](https://g.redditmedia.com/Opizo6PEpuT_cL0N0tWK5g59CsMHFystdNCpYOqhu-A.gif?w=884&fm=mp4&mp4-fragmented=false&s=222875e5455c31829929add6c426a86b)

```{r}
ggplot(simpsons_paradox, aes(x = x, y = y, colour = dataset)) +
    geom_point() +
    geom_smooth(method = "lm") +
    theme(legend.position = "none") +
    facet_wrap( ~ dataset, ncol = 3)
```

## Resources

- https://stats.stackexchange.com/questions/81000/calculate-coefficients-in-a-logistic-regression-with-r
- https://www.autodeskresearch.com/publications/samestats
