---
title: "Statistical/probabilistic modelling in R"
author: "Luke Johnston"
---

## Lesson preamble

> ### Lesson objectives:
> 
> - Learn how to apply and interpret linear regression for a variety of data
> - Understand probability and the importance of presenting confidence intervals
> - Learn the importance of visualizing your data when doing any analyses or
> statistics
> - Learn how to compare models using information criterion techniques
> 
> ### Lesson outline:
> 
> - Statistics and probability
> - Linear regression 
>     - Types of dependent ($y$) data
>     - Types of independent ($x$) data
> - Estimation vs hypothesis testing
> - Model comparison techniques
> 
> ### Setup
>
> - `install.packages("dplyr")` # or `tidyverse`
> - `install.packages("tidyr")` # or `tidyverse`
> - `install.packages("broom")`
> - `install.packages("datasauRus")`

-----

```{r, results='hide'}
library(tidyverse)
library(broom)
```

## Statistics and probability

- Theoretical models are powerful tools at explaining or understanding the
world. However, they are limited in that the real-world often doesn't perfectly
fit these models. The real world is messy and noisy. We use statistics and
probability to determine whether what we are studying is a really



## Generalized linear models

Generalized linear models (or GLM) is statistical modelling technique that
encompasses several other techniques, e.g. correlation or analysis of variance
(ANOVA), linear regression, logistic regression. GLM is a powerful technique
that you can use on a wide range of data and research questions, and is the
foundation for understanding how more advanced techniques work. This is the
reason I will be covering GLM in depth. (For those who will go into graduate
students or into research in general, you will likely use mixed effects models.
If you can understand and grasp GLM, it will be substantially easier for you to
understand mixed effects modelling and other techniques.)

A version of GLM that uses continuous $y$ values is called linear regression,
which I'm going to focus on. The formula for linear regression (or GLM in
general) is:

$$ Y = \alpha + X\beta + \varepsilon $$

Or, a simplified, alphabetized version is:

$$ y = a + Xb + e $$

In the case of multiple linear regression (more than one $x$), it is expanded to:

$$ y = a + x_1b_1 + x_2b_b +...+ x_nb_n + e $$

Where each $x$ is a variable in a data frame. In R we can run linear regression
either using `lm` or using `glm`. We'll use `glm` since it can be expanded to
other types of analyses. First, let's load up a dataset.

```{r}
library(tidyverse)
portal <- read_csv("portal_data.csv")
```

```{r}
fit1 <- glm(weight ~ sex, data = portal, family = gaussian)
summary(fit1)
```

There's a lot of results and information contained within `fit1`, which
`summary()` extracts and presents in a fairly nice format. However, a lot of
this information we aren't really interested in. So let's extract only what we
want, using the `broom` package. (get it, broom to clean up?)

```{r}
library(broom)
tidy(fit1)
```

Much nicer! Ok, so what does this actually mean? Let's go back to the equation
and convert the above into the formula:

$$ weight = Intercept + (sexM \times b) $$

Let's substitute in the numbers. The intercept, is well, the intercept in the
data above. The $b$ is the estimate for the `year`.

$$ weight = 42.17 + (sexM \times 0.825) $$

We can use this equation to estimate the value of weight for a specific sex. But
first, let's make sure this is working correctly. Let's find out the weight at
the very first collection visit.

```{r}
portal %>% 
    filter(!is.na(sex)) %>% 
    group_by(sex) %>% 
    summarize(mean_wt = mean(weight, na.rm = TRUE)) 
```

Ok, let's compare to the equation. The variable `sexM` is 1 for male and 0 for female. So if we want to calculate the weight of female, we have to set `sexM` as 0 in the equation. Let's do it:

```{r}
42.17 + (0 * 0.825)
```

Same as above! And for males (equal to 1):

```{r}
42.17 + (1 * 0.825)
```

Good, it matches what we calculated. Why is this useful? Well, we can add more
terms to the GLM equation. This allows us to determine how a variable influences
the $y$ when other variables are held constant. Since weight is probably also
dependent on `hindfoot_length`, let's add that to the model and put the values
into the equation.

```{r}
fit2 <- glm(weight ~ sex + hindfoot_length, data = portal)
tidy(fit2)
```

$$ weight = -32.09 + (sexM \times -1.72) + (hindfoot\_length \times 2.56)$$

Hmm, that's interesting... is a *negative* intercept... How can weight be
negative? Well, that's because the intercept is calculated by setting *both* sex
and hindfoot length to zero! But is physically possible that a hindfoot is
non-existent?? No. But math doesn't care what is physically possible or not. So
let's change that up by changing the actual value of hindfoot length in the
dataset. There are several ways to transform the data into something that is
meaningful to us. But first:

> What does everyone think is a meaningful way to transform hindfoot length so
we can interpret the model more appropriately?

There are many ways to do it, but one way that is often used is to mean center
the data. Mean centering is substracting each value by the mean. After mean
centering, 0 is equal to the mean of hindfoot length.

```{r}
portal2 <- portal %>%
    mutate(hfl_mean_center = hindfoot_length - mean(hindfoot_length, na.rm = TRUE))

fit3 <- glm(weight ~ sex + hfl_mean_center, data = portal2)
tidy(fit3)
```

$$ weight = 42.88 + (sexM \times -1.72) + (hfl\_mean\_center \times 2.56)$$

Much better. So if we wanted to compare males vs females who have a mean
hindfoot length (set to 0):

```{r}
# Female
42.88 + (0 * -1.72) + (0 * 2.56)
# Male
42.88 + (1 * -1.72) + (0 * 2.56)
```
Or if we wanted to see males with hindfeet 10 cm minus the mean vs 10 cm plus the mean:

```{r}
# Males 10 cm minus mean
42.88 + (1 * -1.72) + (-10 * 2.56)
# Male 10 cm plus mean
42.88 + (1 * -1.72) + (10 * 2.56)
```

#### Challenge

Create a new dataframe called `challenge1` that keeps only taxa of "Rodent"
(`filter`) and with a new column (`mutate`) called "Dipodomys" where if genus
is equal to "Dipodomys" than the value is "Yes" and if not the value is "No"
(hint: use `ifelse`). Then, create a model called `fit_challenge1` that has the
terms `sex`, `hfl_mean_center`, and `Dipodomys`. Extract the relevant
information from the model using `tidy`. What is the weight when sex is female,
hindfoot length is 5 above the mean, and Dipodomys is yes?

```{r, echo=FALSE, eval=FALSE}
challenge1 <- portal2 %>% 
    filter(taxa == "Rodent") %>% 
    mutate(Dipodomys = ifelse(genus == "Dipodomys", "Yes", "No"))

fit_challenge1 <- glm(weight ~ sex + hfl_mean_center + Dipodomys, data = challenge1)
tidy(fit_challenge1)
```


### Example: Using linear regression in a Ricker model?

## Estimation, confidence intervals, and p-values

P-values vs estimation

confidence interval

<!-- how many know about p-values etc? -->

## Model comparison techniques

### Information criterion

## Visualization as a crucial component to data analysis

Visualizing your data before, during, and after doing statistics or modelling
your data is an incredibly important and sometimes overlooked aspect of data
analysis. Sometimes statistics and modelling give you an answer that you sort of
expect, but the actual data is telling a different story.

#### Challenge

There are several datasets found within the `datasauRus` package. This package
is trying to illustrate a point. First, let's look at the mean and standard
deviation of the variables in each dataset:

```{r}
library(datasauRus)
library(tidyverse)
library(broom)
datasaurus_dozen %>%
    group_by(dataset) %>%
    summarise(
        mean_x = mean(x),
        sd_x = sd(x),
        mean_y = mean(y),
        sd_y = sd(y)
    )
```

They're all basically the same... What about for linear regression?

```{r}
# Linear regression on each dataset
datasaurus_dozen %>% 
    group_by(dataset) %>% 
    do(tidy(lm(y ~ x, data = .))[2, ])
```

Same thing, all are more or less the same. But, there is something seriously
wrong with every since one of the datasets. Pair up and explore the data
visually to see what's going on.

```{r, echo=FALSE, eval=FALSE}
ggplot(datasaurus_dozen, aes(x = x, y = y, colour = dataset)) +
    geom_point() +
    theme_void() +
    theme(legend.position = "none") +
    facet_wrap(~ dataset, ncol = 3)
```


### Other examples of finding a problem by visualizing

From what you found in the challenge above, can you see why it's dangerous to
not visualize the data first? Sometimes

![Visualizing raw data vs aggregate](https://g.redditmedia.com/Opizo6PEpuT_cL0N0tWK5g59CsMHFystdNCpYOqhu-A.gif?w=884&fm=mp4&mp4-fragmented=false&s=222875e5455c31829929add6c426a86b)

### Another example: Simpson's Paradox[^simpson]

You all have already encountered Simpson's Paradox in the challenges of lecture
4 and 5, where the average weight of all species decreased over time, but for
each species weight remained constant. These types of problems are encountered
all the time in data analysis and it is part of what makes science so hard,
because data often doesn't behave as we expect it to. 

And sometimes, by not looking at the raw data, it can lead to actual harmful
effects. For instance, Simpson's paradox comes up in medical studies of drugs
that could be lifesaving for a patient. If inappropriate conclusions are drawn
because the data wasn't directly looked at, people can be harmed. This is true
in *any* scientific field, including ecology. Policies aimed at benefitting the
environment could in fact be *harming* it because these types of things were not
examined. See the figure below for an illustration of Simpson's paradox.

```{r}
ggplot(simpsons_paradox, aes(x = x, y = y, colour = dataset)) +
    geom_point() +
    geom_smooth(method = "lm") +
    theme(legend.position = "none") +
    facet_wrap( ~ dataset, ncol = 3)
```

[^simpson]: Wikipedia page on [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)

## Resources

- https://stats.stackexchange.com/questions/81000/calculate-coefficients-in-a-logistic-regression-with-r
- https://www.autodeskresearch.com/publications/samestats
